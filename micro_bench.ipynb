{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e67966",
   "metadata": {},
   "source": [
    "# Mirco-Benchmarking for Transformers\n",
    "\n",
    "This notebook benchmarks the most time consuming components in BERT, GPT-2 and T5 to help you understand its performance. Let's first check our libraries and hardware. If your GPUs are recent models, please make sure your CUDA version is also recent, which may greatly affect the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65782c24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version\t: 2.0.1\n",
      "CUDA version\t: 11.8\n",
      "GPU\t\t: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print('Pytorch version\\t:', torch.__version__)\n",
    "print('CUDA version\\t:', torch.version.cuda)\n",
    "print('GPU\\t\\t:',torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288535a8",
   "metadata": {},
   "source": [
    "Let's first define a `walltime` method to benchmark Pytorch statements by at least 3 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d06ae2d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from torch.utils import benchmark \n",
    "from IPython.display import clear_output\n",
    "\n",
    "pd.options.display.precision = 3\n",
    "\n",
    "def var_dict(*args):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    return dict([(name, val) for name, val in callers_local_vars if val is arg][0] \n",
    "                for arg in args)\n",
    "\n",
    "def walltime(stmt, arg_dict, duration=3):\n",
    "    return benchmark.Timer(stmt=stmt, globals=arg_dict).blocked_autorange(\n",
    "        min_run_time=duration).median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b1141",
   "metadata": {},
   "source": [
    "Last install huggingface from source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcd79038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!git clone https://github.com/huggingface/transformers\n",
    "!cd transformers; pip install .\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d00a71",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "Matrix multiplication is the most used operator in Transformers. Its performance is crucial. Let's test the [TFLOPS](https://en.wikipedia.org/wiki/FLOPS) we can achieve on square matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3ca0f77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m matmul_tflops \u001b[38;5;241m=\u001b[39m \u001b[43mdefaultdict\u001b[49m(\u001b[38;5;28;01mlambda\u001b[39;00m: {})\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m8192\u001b[39m]:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dtype \u001b[38;5;129;01min\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mfloat32, torch\u001b[38;5;241m.\u001b[39mfloat16):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "matmul_tflops = defaultdict(lambda: {})\n",
    "for n in [128, 512, 2048, 8192]:\n",
    "    for dtype in (torch.float32, torch.float16):\n",
    "        a = torch.randn(n, n, dtype=dtype).cuda()\n",
    "        b = torch.randn(n, n, dtype=dtype).cuda()   \n",
    "        t = walltime('a @ b', var_dict(a, b))\n",
    "        matmul_tflops[f'n={n}'][dtype] = 2*n**3 / t / 1e12\n",
    "        del a, b\n",
    "        \n",
    "pd.DataFrame(matmul_tflops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f292a91",
   "metadata": {},
   "source": [
    "You can see that the performance increases with the matrix size. If your GPU has [Tensor Cores](https://www.nvidia.com/en-us/data-center/tensor-cores/), you will see a big performance jump when switching from 32-bit floating points to 16-bit floating points.\n",
    "\n",
    "Next you can find the theory TFLOPS of your GPU from Wikipedia, for example, [Nvidia Tesla](https://en.wikipedia.org/wiki/Ampere_(microarchitecture)), [Nvidia Quadro](https://en.wikipedia.org/wiki/Quadro), [RTX 30xx](https://en.wikipedia.org/wiki/GeForce_30_series), and [RTX 20xx](https://en.wikipedia.org/wiki/GeForce_20_series). Here we list several cards, with their memory information.\n",
    "\n",
    "| Model       | Memory (GB) | Memory Bandwidth (GB/sec) | FP32 TFLOPS | FP16 TFLOPS |\n",
    "| ----------- | ----------- | ------------------------- | ----------- | ----------- |\n",
    "| A100        | 80          | 2039                      | 19.5        | 312         |\n",
    "| V100        | 16          | 900                       | 15.7        | 125         |\n",
    "| A6000       | 48          | 768                       | 38          | 150         |\n",
    "| RTX 3090 TI | 24          | 1008                      | 40          | 160         |\n",
    "\n",
    "If the best TFLOPS number you got is still far away from the theory TFLOPS of your GPU, the performance is likely bottlenecked by the memory bandwidth. To illustrate it, let's benchmark a simple elemental-wise multiplication to show both its TFLOPS with memory bandwidth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6809d73e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>65536</th>\n",
       "      <th>262144</th>\n",
       "      <th>1048576</th>\n",
       "      <th>4194304</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TFLOPS</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB/s</th>\n",
       "      <td>17.720</td>\n",
       "      <td>71.164</td>\n",
       "      <td>280.222</td>\n",
       "      <td>1151.771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        65536    262144   1048576   4194304\n",
       "TFLOPS    0.002    0.009    0.035     0.144\n",
       "GB/s     17.720   71.164  280.222  1151.771"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = defaultdict(lambda: {})\n",
    "for n in [1024*64, 1024*256, 1024*1024, 1024*1024*4]:\n",
    "    a = torch.randn(n).cuda()\n",
    "    t = walltime('a * 1.2', var_dict(a))\n",
    "    vector[n]['TFLOPS'] = n / t / 1e12\n",
    "    vector[n]['GB/s'] = 8 * n / t / 1e9\n",
    "    \n",
    "pd.DataFrame(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec285e5f",
   "metadata": {},
   "source": [
    "You can see that even for large vectors, the TFLOPS is far far way from GPU peak performance, while the bandwidth may be quite close to its theoretical number.\n",
    "\n",
    "The matrix multiplication performance is a main topic in HPC. There are a large number of research papers. Unfortunately the backend library, cuBLAS, is not open sourced. You may check [cutlass](https://github.com/NVIDIA/cutlass), which claimed similar performance as cuBLAS, for some implementation details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c13b71",
   "metadata": {},
   "source": [
    "## BERT Layer\n",
    "\n",
    "The main body of a Transformer model is a stacking of Transformer blocks. Let's benchmark the performance of a single block. In BERT, it is often called a BERT layer. Let's construct one such layer from the [BERT large model](https://huggingface.co/bert-large-uncased). We use 16-bit floating points for better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c9957b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/local/miniconda3/envs/gpt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-15 22:13:41,752] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, BertLayer\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"bert-large-uncased\")\n",
    "layer = BertLayer(config).half().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2315ab",
   "metadata": {},
   "source": [
    "Then define a function to benchmark both forward and forward with backward performance using different sequence lengths and batch sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f7f89c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def layer_benchmark(layer, hidden_size, seq_lens, batch_sizes, cross_attention=False):\n",
    "    h = hidden_size\n",
    "    results = defaultdict(lambda: {})    \n",
    "    encoder_state = 'encoder_hidden_states=X' if cross_attention else ''\n",
    "    for s in seq_lens:\n",
    "        for b in batch_sizes:            \n",
    "            ffn = 16*b*s*h*h / 1e12  # TFLOPS for the Feed-Forward Network\n",
    "            atten = (4*b*h*s*s + 8*b*s*h*h) / 1e12  # TFLOPS for attention            \n",
    "            forward = ffn + (2 if cross_attention else 1) * atten\n",
    "            \n",
    "            X = torch.randn(b, s, h).half().cuda()\n",
    "            results[f'batch={b}'][f'fwd seq_len={s}'] = forward / walltime(\n",
    "                f'layer(X, {encoder_state})', var_dict(layer, X))\n",
    "            results[f'batch={b}'][f'fwd+bwd seq_len={s}'] = 3 * forward / walltime(\n",
    "                f'layer(X, {encoder_state})[0].sum().backward()', var_dict(layer, X))            \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9116be57",
   "metadata": {},
   "source": [
    "In BERT pre-training, we often train with a sequence of 128 (stage 1) or 512 (stage 2). Let's test its performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e278b06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch=2</th>\n",
       "      <th>batch=4</th>\n",
       "      <th>batch=8</th>\n",
       "      <th>batch=16</th>\n",
       "      <th>batch=32</th>\n",
       "      <th>batch=64</th>\n",
       "      <th>batch=128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fwd seq_len=128</th>\n",
       "      <td>3.263</td>\n",
       "      <td>6.746</td>\n",
       "      <td>13.634</td>\n",
       "      <td>26.817</td>\n",
       "      <td>52.053</td>\n",
       "      <td>98.295</td>\n",
       "      <td>99.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd+bwd seq_len=128</th>\n",
       "      <td>3.658</td>\n",
       "      <td>7.406</td>\n",
       "      <td>14.697</td>\n",
       "      <td>29.352</td>\n",
       "      <td>63.848</td>\n",
       "      <td>114.903</td>\n",
       "      <td>103.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd seq_len=512</th>\n",
       "      <td>14.245</td>\n",
       "      <td>28.202</td>\n",
       "      <td>55.251</td>\n",
       "      <td>74.297</td>\n",
       "      <td>72.052</td>\n",
       "      <td>69.830</td>\n",
       "      <td>70.109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd+bwd seq_len=512</th>\n",
       "      <td>15.952</td>\n",
       "      <td>32.885</td>\n",
       "      <td>68.520</td>\n",
       "      <td>81.532</td>\n",
       "      <td>77.359</td>\n",
       "      <td>79.215</td>\n",
       "      <td>79.371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     batch=2  batch=4  batch=8  batch=16  batch=32  batch=64  \\\n",
       "fwd seq_len=128        3.263    6.746   13.634    26.817    52.053    98.295   \n",
       "fwd+bwd seq_len=128    3.658    7.406   14.697    29.352    63.848   114.903   \n",
       "fwd seq_len=512       14.245   28.202   55.251    74.297    72.052    69.830   \n",
       "fwd+bwd seq_len=512   15.952   32.885   68.520    81.532    77.359    79.215   \n",
       "\n",
       "                     batch=128  \n",
       "fwd seq_len=128         99.165  \n",
       "fwd+bwd seq_len=128    103.707  \n",
       "fwd seq_len=512         70.109  \n",
       "fwd+bwd seq_len=512     79.371  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_benchmark(layer, config.hidden_size, [128, 512], [2, 4, 8, 16, 32, 64, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889961fa",
   "metadata": {},
   "source": [
    "No surprise that a large batch size helps. But the best number is below the matrix multiplication TFLOPS. Let's find why.\n",
    "\n",
    "We first benchmark the first dense layer in the Feed-Forward Network (FFN) in the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c39f6f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dense layer TFLOPS: 162.399'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h, b, s = config.hidden_size, 64, 128\n",
    "X = torch.randn(b, s, h).half().cuda()\n",
    "\n",
    "'Dense layer TFLOPS: %.3f' % (8*b*s*h*h / 1e12 / walltime(    \n",
    "    'layer.intermediate.dense(X)', var_dict(layer, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea6579",
   "metadata": {},
   "source": [
    "The number is pretty good. Then run this dense layer with the GeLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44620688",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dense+Activation TFLOPS: 127.706'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Dense+Activation TFLOPS: %.3f' % (8*b*s*h*h / 1e12 / walltime(\n",
    "    'layer.intermediate(X)', var_dict(layer, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591ed3c",
   "metadata": {},
   "source": [
    "Even the activation function has a ignorable complexity, it brings down the TFLOPS. We pointed out the reason before, the elemental-wise operation of the activation function is bounded by the memory bandwidth.\n",
    "\n",
    "Now test the whole FFN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6837160",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FFN TFLOPS: 137.124'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = 16*b*s*h*h / 1e12\n",
    "'FFN TFLOPS: %.3f'%(ffn / walltime(\n",
    "    'layer.output(layer.intermediate(X),X)', var_dict(layer, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59214b42",
   "metadata": {},
   "source": [
    "The other part in the BERT layer is the multi-head self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2b4e48d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attention TFLOPS: 50.862'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = (4*b*h*s*s + 8*b*s*h*h) / 1e12\n",
    "'Attention TFLOPS: %.3f'%(\n",
    "    att / walltime('layer.attention(X)', var_dict(layer, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eec79b",
   "metadata": {},
   "source": [
    "Even though the main computation part of the attention block is still matrix multiplication, it has more memory bounded operators compared to FFN. So you see a lower TFLOPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02d0e4df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53125"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att / ffn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daaaf4e",
   "metadata": {},
   "source": [
    "The ratio of complexity between attention and FFN depends on the BERT configuration. The overall performance is a weighted sum between the FLOPS of these two components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32888ee3",
   "metadata": {},
   "source": [
    "## GPT-2 Block\n",
    "\n",
    "Next let's evaluate `gpt2-medium`, which has a similar architecture has `bert-large`, i.e. 24 layers with a 1024 hidden size. GPT2 is trained with a 1024 sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f889cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|████████████████████████████████████████████| 718/718 [00:00<00:00, 3.41MB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch=2</th>\n",
       "      <th>batch=4</th>\n",
       "      <th>batch=8</th>\n",
       "      <th>batch=16</th>\n",
       "      <th>batch=32</th>\n",
       "      <th>batch=64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fwd seq_len=512</th>\n",
       "      <td>12.955</td>\n",
       "      <td>26.120</td>\n",
       "      <td>48.972</td>\n",
       "      <td>49.512</td>\n",
       "      <td>48.461</td>\n",
       "      <td>48.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd+bwd seq_len=512</th>\n",
       "      <td>15.013</td>\n",
       "      <td>33.425</td>\n",
       "      <td>55.117</td>\n",
       "      <td>55.289</td>\n",
       "      <td>54.122</td>\n",
       "      <td>54.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd seq_len=1024</th>\n",
       "      <td>27.231</td>\n",
       "      <td>43.034</td>\n",
       "      <td>39.354</td>\n",
       "      <td>38.792</td>\n",
       "      <td>38.685</td>\n",
       "      <td>38.617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd+bwd seq_len=1024</th>\n",
       "      <td>32.080</td>\n",
       "      <td>47.361</td>\n",
       "      <td>44.723</td>\n",
       "      <td>44.058</td>\n",
       "      <td>44.385</td>\n",
       "      <td>44.367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      batch=2  batch=4  batch=8  batch=16  batch=32  batch=64\n",
       "fwd seq_len=512        12.955   26.120   48.972    49.512    48.461    48.229\n",
       "fwd+bwd seq_len=512    15.013   33.425   55.117    55.289    54.122    54.566\n",
       "fwd seq_len=1024       27.231   43.034   39.354    38.792    38.685    38.617\n",
       "fwd+bwd seq_len=1024   32.080   47.361   44.723    44.058    44.385    44.367"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"gpt2-medium\")\n",
    "layer = GPT2Block(config, layer_idx=0).half().cuda()\n",
    "layer_benchmark(layer, config.n_embd, [512, 1024], [2, 4, 8, 16, 32, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c8cd5",
   "metadata": {},
   "source": [
    "You can see that, despite GPT-2 and BERT has the same complexity, GPT-2 has slightly worse TFLOPS when using the same batch size and sequence length. Also using a larger sequence length 1024 further harms the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e285d9d",
   "metadata": {},
   "source": [
    "## T5 Layer\n",
    "\n",
    "T5 has both encoder and decoder, let's first benchmark the decoder, whose performance is similar to BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74231af6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|████████████████████████████████████████| 1.21k/1.21k [00:00<00:00, 5.54MB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch=2</th>\n",
       "      <th>batch=4</th>\n",
       "      <th>batch=8</th>\n",
       "      <th>batch=16</th>\n",
       "      <th>batch=32</th>\n",
       "      <th>batch=64</th>\n",
       "      <th>batch=128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fwd seq_len=512</th>\n",
       "      <td>10.833</td>\n",
       "      <td>21.604</td>\n",
       "      <td>45.288</td>\n",
       "      <td>48.222</td>\n",
       "      <td>48.051</td>\n",
       "      <td>46.946</td>\n",
       "      <td>46.672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd+bwd seq_len=512</th>\n",
       "      <td>15.353</td>\n",
       "      <td>27.438</td>\n",
       "      <td>55.059</td>\n",
       "      <td>56.252</td>\n",
       "      <td>54.679</td>\n",
       "      <td>55.196</td>\n",
       "      <td>55.053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     batch=2  batch=4  batch=8  batch=16  batch=32  batch=64  \\\n",
       "fwd seq_len=512       10.833   21.604   45.288    48.222    48.051    46.946   \n",
       "fwd+bwd seq_len=512   15.353   27.438   55.059    56.252    54.679    55.196   \n",
       "\n",
       "                     batch=128  \n",
       "fwd seq_len=512         46.672  \n",
       "fwd+bwd seq_len=512     55.053  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.t5.modeling_t5 import T5Block\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"t5-large\")\n",
    "config.use_cache = False\n",
    "config.is_decoder = False\n",
    "config.is_encoder_decoder = False\n",
    "\n",
    "encoder = T5Block(config).half().cuda()\n",
    "layer_benchmark(encoder, config.d_model, [512], [2, 4, 8, 16, 32, 64, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19679764",
   "metadata": {},
   "source": [
    "The decoder has an additional cross attention, which increases the time complexity and also hurts TFLOPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a57c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config.is_decoder = True\n",
    "decoder = T5Block(config).half().cuda()\n",
    "layer_benchmark(decoder, config.d_model, [512], [2, 4, 8, 16, 32, 64, 128], cross_attention=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a2765",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "To conclude, to achieve the best performance for a Transformer layer, you need to use a fast data type and a large batch size. For further improvement, we may need to rewrite the code. For example, [fusing](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#fuse-pointwise-operations) multiple kernels into a single one. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
